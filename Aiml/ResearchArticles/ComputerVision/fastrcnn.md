# Fast R-CNN

## Introduction

Object detection is a more challenging task in comparison to image classification. This is due to the requirement of accurate localization of objects.

The localization problem arises due to:

- Several object proposals need to be processed due to existence of numerous objects in a provided image.
- The provided candidates are only rough localization estimates that must be refined to achieve precise localization.

Fast R-CNN aims to address these issues and streamline the training process with a **single-stage** training algorithm that **jointly** learns to classify object proposals and refine their localization estimates. 

Fast R-CNN can train a very deep detection network like VGG16 9 times faster than R-CNN.

### R-CNN and SPPnet

While R-CNN achieves good object detection accuracy, it suffers from the following drawbacks:

- **Training is a multi-stage pipeline**: R-CNN first fine tunes a CNN on object proposals(this includes initial training on a wide-spectrum auxilary dataset followed by fine tuning on a domain specific dataset). Thereafter, per-class linear SVMs are fit on the features generated by the CNN. The SVMs act as the object detectors that replace the softmax classifiers learnt by fine tuning. Finally, in the third stage of training, bounding box regressors are learnt.
- **Training is expensive in space and time**: For SVM and bounding-box regressor training, features that are extracted from each object proposal in each image is written to disk.
- **Object detection is slow**

In R-CNN, for an image, all its object proposals undergo a forward pass through the CNN without sharing of any computation. This accounts for the slow performance of R-CNN. 

Spatial pyramind pooling networks implement sharing of the computation for R-CNN by first computing a convolutional feature map on the entire image. It then classifies each object proposal using a feature vector extracted from the shared feature map. Features are extracted for a proposal by max-pooling the portion of the feature map inside the proposal into a fixed-size output. Multiple output sizes are pooled and then concatenated as in spatial pyramind pooling. While SPPnet accelerates R-CNN by 10 to 100 times at test time and reduces training time to one-third, it suffers from the drawback of a multi-stage pipeline. Another notable drawback is that the fine tuning algorithm proposed in SPPnet cannot update the convolutional layers that precede the spatial pyramid pooling. This limits the accuracy when using very deep networks.

### Contributions

The advantages of Fast R-CNN are:

* Higher mAP than R-CNN and SPPnet
* Training is single-stage using a multi-task loss
* Training can update all network layers.

## Fast R-CNN architecture and training

An overview of the Fast R-CNN architecture is:

* It takes as input an entire image and a set of object proposals.
* The network first processes the **whole image** with several convolutional and max pooling layers to produce a **feature map**.
* Then, **for each object proposal**  a **region of interest pooling layer** extracts a fixed-length **feature vector** from the feature map.
* Each feature vector is fed into a sequence of fully connected layers that finally branch into **two sibling output layes**: one that produces softmax probability estimates over the K object classes and one background class and another layer that provides the bounding box refinement through four real-valued numbers for each class.

### The ROI pooling layer

This layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of HxW. An ROI is a rectangular window into a convolutional feature map received from the convolutions applied on the entire image. Each ROI is specified by a four tuple that specifies the top left corner of the window and its height and width. Max pooling is carried out on a window of size hxw by dividing it into HxW sub-windows and then max pooling the values in each sub-window (typical max pooling). ROI pooling is a special case of the spatial pyramid pooling layer used in SPPnets with only one pyramid level.

### Initializing from pre-trained networks

When using a pre-trained network, the following three changes are made to the network architecture:

* The last max pooling layer is replaced by the ROI pooling layer.
* The last fully connected and softmax layers are replaced with the two sibling layers.
* The network is modified to take two data inputs, a list of images and a list of ROIs in those images.

### Fine-tuning for detection

Training all network weights with back propagation is an important capability of Fast R-CNN. **Stochastic gradient descent mini batches are sampled hierarchially**, first by sampling N images and then by sampling R/N ROIs from each image. ROIs from the same image share computation and memory in the forward and backward passes.

In addition to hierarchial sampling, Fast R-CNN uses a streamlined training process with one fine-tuning stage that jointly optimizes a softmax classifier and bounding-box regressors 

#### Multi-task loss

Fast R-CNN has two sibling output layers.

* The first outputs a discrete probability distribution per ROI over the K+1 categories p(p_0, ...p_k). It is calculated as a softmax applied over the K+1 outputs of a fully connected layer.
* The second ouputs bounding-box regression offsets as a 4-tuple t^k = (t^k_x, t^k_y, t^k_w, t^k_h) for **each of the K object classes**.

Each training ROI is labelled by a ground-truth class u and a ground-truth bounding-box regression target v. A multi task loss is used on each labeled ROI to jointly train for classification and bounding-box regression. The background is given the class u=0.

L(p, u, t^u, v) = L_(cls)(p, u) + lambda[u>=1]L_(loc)(t^u, v)

where L_(cls)(p, u) = -log(p_u)

The loss L_(loc) is defined over the 4 tuple of the bounding box regression. It is a summed smooth L1 loss difference between the coordinates between the ground truth regression-box values v and the predicted regression-box value t^u.

The hyperparameter lambda is used to control the balance between the two task losses.

#### Mini-batch sampling

For stochastic gradient descent, each mini-batch comprises of 2 images with 64 ROIs taken from each image. 25% of the ROIs were taken from object proposals that have an IOU overlap with the ground-truth bounding boxes of at least 0.5. These represent that K object classes. The rest were taken from ROIs with IOU overlap with ground-truth bounding boxes of less than 0.5 and at least 0.1. These represent the background class instances.

## Fast R-CNN detection

At test time, an image and a list of R object proposals are provided as input with R typically being 2000.

For each test ROI r, the forward pass

* outputs a class posterior probability distribution p and a set of predicted bounding-box offsets relative to r **for each of the K classes**. The estimated probabilityu P(class = k|r) = p_k is assigned as a **detection confidence to r for each object class k**. Non-max suppression is then applied to the classes as in R-CNN.

### Truncated SVD for faster detection

In whole-image classification, time spent in computation by the fully connected layers is small compared to that of the convolution layers. In object detection, due to large number of ROIs, almost half the computation time in the forward pass is spent on the fully connected layers.

Large fully connected layers can be accelarated by truncated SVD.

A u x v weight matrix W can be decomposed as:

W = UDV^T

where U is a u x t matrix with the top t left eigen vectors, D is a diagonal matrix of t eigen values and V is a v x t matrix of the top t right eigen vectors.
This allows to reduce the parameter count from u*v to t*(u+v) which can be a significant reduction when t is much smaller than u or v.

To compress the network, the fully connected layer corresponding to W is broken up into 2 fully connected layers without a non-linearity between them. The first matrix uses the weight matrix DV^T(without biases) and the second uses U(with the biases of W). This provides good speedups when the number of ROIs is large.

## Main results

Truncated SVD can reduce detection time by more than 30% with only a small drop in mAP and without needing to perform addititional fine-tuning after model compression(i.e. separating the fully connected layers into the two constituent layers). Further speed-ups are possible with smaller drops in mAP if one fine-tunes again after compression.

### Which layers to fine-tune?

For less deep network, only fine-tuning the fully connected layers may be sufficient for good accuracy. However, in the event of deeper networks like VGG16, fine-tuning by freezing all the convolutional layers to only fine-tune the fully connected layers simulated single-scale SPPnet trainig and decreased mAP. Thus, training through the ROI pooling layer is important for very deep networks.

In smaller networks, the earliest convolutions are generic and task independant and so, fine-tuning these early layers has no meaningful effect on the mAP.

###### Tags

:computervision:objectdetection:rcnn:fastrcnn:
