# Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment

## Abstract

* Electronic health records make it possible to learn accurate patient representations but doctor representations are not well studied.

* Doctor2Vec simultaneously learns:
	* doctor representations from EHR data
	* trial representations from the description of categorical information about the trials

* Uses a dynamic memory bank storing doctor experience with patients and a network dynamically assigns weights based on the trial representation via attention.

## Introduction

* Doctors play a pivotal role in connecting patients to treatments.

* The method captures time-evolving patterns of doctors experience and expertise encoded in EHR data of patients.

* It uses a dynamic doctor representation rather than a static one.

* **The method uses patient embedding as a memory for dynamic doctor representation learning. The doctors' evolving experience is based on the representation from the doctors' patients.**

* **Trial embeddings are used as queries for improved doctor selection.**

## Method

### Problem Formulation

* **Doctor Records**:
	* For each doctor, the **clinical experience** can be represented as a sequence of patients that the doctor has seen as:
		
		$C(m) = {P_1^(m), P_2^(m), ... P_k^(m)}$
	  
	  for the $mth$ doctor with $k$ patients.
	  
	* Each patient can be represented as a sequence of visits as:
		
		$P(k) = P{v_1^k, v_2^k, ... v_T^k}$
		
	  for the $kth$ patient of a doctor having $T$ visits.
	  
	* Each visit $v_i^k$ is a combination of diagnosis codes, procedure codes and medication codes which are each **multi-hot vectors**
	
* **Clinical Trial Data**:
	* Clinical trial data consists of:
		* **trial descriptions in unstructured text**
		* **categorical features such as trial phase, primary indication**

	* Each clinical trial is a combination of the above two as:
		
		$Q(cat)(l) = {f_1^(l), ... f_v^(l)}$
		$Q(l) = [Q(cat)(l); Q(text)(l)]$
		
		for the $lth$ clinical trial with categorical features $f_i$.
		
### Doctor2Vec Framework

* **Hierarchial Patient Embedding**:
	* A doctor's experience is modeled as a function over the embeddings of their patients' EHR data.
	* For patients consulting two or more doctors, their corresponding EHR data embedding is used for both doctors separately.

	* Patient representations are learnt by leveraging the inherent multilevel structure of EHR data. The hierarchial structure includes patients on top, followed by visits of the patients and the set of diagnosis, procedure and medication codes at the leaf nodes.

	* For a clinical visit of a patient, $v_t^(k)$, a visit embedding $h_t^(k)$ is produced by passing through an MLP as:
		
		$h_t^(k) = W_emb v_t^(k)$
		
	* The patient embedding based on a sequence of visits is obtained by passing the visit embeddings $h_t$ through a biLSTM network an attention layer on to prioritize important visits. The context vector provided by the attention mechanism as a weighted sum of the patient visits gives the patient representation.

* **Multimodal Trial Information Embedding**:
	* Collected trial data comprises of trial descriptions as unstructured texts and categorical features. These are embedded into a shared space by a multi-modal method.
		
		* **Unstructured Text**: For textual descriptions of trials, BERT model is used. **The word level BERT embeddings of a textual description are averaged to get the textual embedding**.

		* **Categorical Features**: The one-hot categorial features are fed through an MLP layer to get the categorical embedding.
		
	* The textual embedding $Q(text)_emb$ and categorical embedding $Q(cat)_emb)$ are **fused by first passing them through individual MLPs and then taking their element-wise product** to get the final clinical trial embedding.

* **Dynamic Doctor Memory Networks**:
	* Given that a doctor sees a variety of patients, the representation of a doctor with respect to a certain trial is to be dynamically constructed. 

	* Using patients' representations as memory vectors and a trial embedding as a query, an attention mechanism is used to fetch relevant patient vectors from the memory bank to dynamically create a doctor representation.

		* **Input Memory Representation**:
			* It converts patient representations to input representations $I_f(k)$ through a dense layer.
		
		* **Generalization**:
			* It refers to the process of updating memory representation for the memory bank. The patients' input representations are used to initialize the memory representation $M_d$ of a doctor.
			* An LSTM layer applied to the input representations is used to update $M_d$ through mulitple iterations.

		* **Output**:
			* The final output memory representation is generated.
			* The relevance between a trial embedding $Q_emb(l)$ and a doctor representation $M_d$ is generated by an attention mechanism using the trial embedding as a query as:
				
				$A(k) = softmax[Q_emb^(l)^T M_d]$
				
		* **Response**:
			* The final embedding $Doc_emb$ is a weighted sum of the input representations, weighted by the Attention weights $A(k)$
		
					  ___	
			Doc_emb = \	  A(k) I_f(k)
					  /__
					  
		* Static information about doctors such as length of practice and education history are also added to the dynamic embedding $Doc_emb$.
		
		* The resulting final embedding vectors are then fed into a fully connected layer and passed through a softmax to obtain class labels.
		
			$Y = softmax([Doc_emb; Q_emb(l);Doc_static])
			
### Training and Inference

* The model is trained by minimizing cross entropy loss.
* Calibrated threshold values are used for obtaining predicted labels from the model output predicted probability values.


