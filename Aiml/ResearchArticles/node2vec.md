# node2vec: Scalable Feature Learning for Networks ## Abstract * node2vec is an algorithmic framework for learning continuous feature representations for nodes in networks.  * node2vec learns a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighbourhoods of nodes.

* Defines a flexible notion of a node's network neighbourhood and designs a **biased random walk procedure** which efficiently explores diverse neighbourhoods.

## Introduction

* In a typical node classification task, the main interest is in predicting the most probable labels of nodes in a network.

* Another task of interest is in link prediction where the objective is in predicting whether a pair of nodes in a network should have an edge connecting them.

* Supervised machine learning algorithms require a set of informative, discriminating and independent features. For prediction problems on networks, this means constructing a feature vector representation for the nodes and edges.

* Typical solutions involves hand-engineering features which is tedious and expensive.

* Another approach involves learning feature representations by solving an optimization problem by balancing the trade-off between computational efficiency and predictive accuracy. While providing good accuracy, it comes at the cost of high training time complexity as several of these methods can involve the eigendecomposition of a large matrix.

* An alternative would be to design an objective that seeks to preserve local neighbourhoods of nodes.

* Nodes in networks can be organized based on:
	* **the communities they belong to or homophily**
	* **structural roles of nodes in the network or structural equivalence**
* **Real-world networks commonly exhibit a mixture of such equivalences**.

* Thus, it is essential to allow for a flexible algorithm that can learn node representations obeying both principles of node structuring.

* node2vec is a **semi-supervised algorithm** for scalable feature learning in networks.

* node2vec optimizes a custom graph-based objective function using SGD.

* node2vec returns feature representations that maximize the likelihood of preserving network neighbourhoods of nodes in a $d$-dimensional feature space.

* It used a $2nd$ order random walk approach to generate network neighbourhoods for nodes.

* node2vec uses biased random walks that can be tuned to explore diverse notions of neighbourhoods.

* Feature representations by node2vec can also be used for edges.

## Feature Learning Framework

* Let $G = (V, E)$ be a given network. Let $f: V -> R^d$ be the required function mapping from the nodes to feature representation space i.e. vectors in $d$-dimensional space. $f$ is a $|V| x d$ weight matrix. For every source node $u in V$, the network neighbourhood of $u$ generated by a sampling strategy $S$ is given by $N_S(u)$.

* node2vec extends on the Skip-gram architecture and defines a similar objective function which is given by:
	
		 ___	
	max  \	 log Pr(N_S(u)|f(u))
	 f	 /__
		u in V 
	
 i.e. **it seeks to maximize the log-probability of observing a network neighbourhood** $N_S(u)$ **for a node u conditioned on its feature representation give by f**
 
* To make the optimization problem tractable, the following two assumptions are made:
	* **Conditional independence**: The likelihood is factorized by assuming that the likelihood of observing a neighbourhood node is independent of observing any other neighbourhood node given the feature representation of the source:
		
						  ___	
		Pr(N_S(u)|f(u)) = | | Pr(n_i|f(u))
						  | |  
				      n_i in N_S(u)
					  
	* **Symmetry in feature space**: A source node and neighbourhood nood have a symmetric effect over each other in the feature space.

## Classic search strategies

* BFS and DFS represent two extreme sampling strategies in terms of the search space they explore.

* Real-world graphs exhibit a combination of both types node structures.

* **BFS**: The neighbourhood of a node is restricted to nodes athat are immediate neighbours of the source. Neighbourhoods sampled by BFS lead to embeddings that corresponds to structural equivalence.

* **DFS**: The neighbourhood consists of nodes sequentially sampled at increasing distances from the source node. DFS allows for exploring larger parts of the network as it can move further away from the source node. In DFS, sampled nodes more accurately reflect a macro-view of the neighbourhood which is essential in inferring communities based on homophily.
## node2vec

### Random Walks

* Given a source node $u$, a random walk of length $l$ is simulated. Nodes are generated by the following distribution:
							   __	
							   |  pi_vx
							   |  -----	if (v, x) in E
    P(c_i = x | c_(i-1) = v) = |    Z
							   |
							   |_	0	otherwise
							   
 where $c_i$ denotes the $ith$ node in the walk starting with $c_0 = u$
       $pi_vx$ is the unnormalized transition probability between nodes $v$ and $x$
	   $Z$ is the normalizing constant
	   
### Search bias alpha

* Biasing the random walks based on the static edge weights does not account for the network structure and guide the search procedure to explore different types of network neigbhourhoods.

* A $2nd$ order random walk with parameters $p$ and $q$ is defined to guide the walk and allow for tuning between the extremities of BFS and DFS.

* If the edge $(t, v)$ is traversed to reside at node $v$, the unnormalized transition probabilities for the next transition is computed from the edges $(v,x)$ leading from the node $v$ as:
	
	$pi_vx = alpha_pq(t,x) w_vx$
					 __	
					 |	 1
					 |	---	if d_tx = 0
	alpha_pq(t, x) = |   p
					 |  
					 |   1	if d_tx = 1
					 |
					 |	 1
					 |	---	if d_tx = 2
					 |_	 q
	
   where $d_tx$ denotes the shortest path distance between nodes $t$ and $x$. $d_tx$ must be within ${0, 1, 2}$
   
* The parameters $p$ and $q$ control how fast the walk explores and leaves the neighbourhood of the starting node. The parameters allow the search procedure to interpolate between BFS and DFS and thereby reflect an affinity for different notions of node equivalences.

### Return parameter p

* The parameter $p$ controls the likelihood of immediately revisiting a node in the walk.

* Setting it to a high value ensures that there is less likeliness to sample an already visited node in the following two steps. This encourages moderate exploration and avoids 2-hop redundancy in sampling.

* Setting it to a low value would lead the walk to backtrack a step and would keep the walk local i.e. close to the starting node.

### In-out parameter q

* The parameter $q$ allows the search to differentiate between inward and outward nodes.

* If $q > 1$, the random walk is biased toward nodes close to node $t$. Such walks obtain a local view of the underlying graph with respect to the start node and approximate BFS behaviour.

* If $q < 1$, the walk is more inclined to visit nodes which are further away from the node $t$. Such behaviour is reflective of DFS which encourages outward exploration. Since DFS-like exploration is achieved through the random walk framework, the sampled nodes are not at strictly increasing distances from a given source node $u$.

* **Setting** $pi_vs$ **to be a function of the preceeding node in the walk** $t$, **the random walks are** $2nd$ **order Markovian**.

### Benefits of random walks


