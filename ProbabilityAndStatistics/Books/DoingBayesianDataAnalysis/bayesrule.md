# Baye's Rule

Baye's rule is the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data.

## Derived from definitions of conditional probability

From the definition of conditional probability, 

p(c|r) = p(r,c)/p(r)

so,

p(r,c) = p(c|r)*p(r)

At the same time,

p(r|c) = p(c,r)/p(c)

so p(c,r) = p(r|c)* p(c)

However, since p(r,c) = p(c,r), we have,

p(c|r)*p(r) = p(r|c)*p(c)

and we have 

p(c|r)  = p(r|c)*p(c)/p(r)
		= p(r|c)*p(c)/sum_c#(p(r|c#)*p(c#))

Baye's rule can be used to go from marginal probabilities to conditional probabilities.
It enables one to go from the prior(marginal) beliefs about something to the posterior(conditional) beliefs.

## Applied to Parameters and Data

Given a model of data that specified the probability of data values given the model's structures and parameter values, Baye's rule allows to convert that into information about how strongly one should believe the various parameter values, given the data:

i.e. given p(data values|parameters values) along with the prior p(parameter values),

one can find p(parameters values|data values)

Therefore, the final form is:

p(theta|data) = p(data|theta)*p(theta)/p(data)

where
p(theta): is the prior distribution or credibility given to the parameter values
p(theta|data): is the posterior distribution or the credibility of theta values with the data taken into account
p(data|theta): is the likelihood or the probability that the data could be generated by the model with the parameter theta
p(data): is the evidence or marginal likelihood or, the overall probability of the data of the model, averaged across all parameter values weighted by the prior strength of belief in those parameter values

### Data-order invariance

If a posterior belief p(theta|D) is made from a prior p(theta), based on some data D, the additional availability of data D' can change the nature of the posterior belief depending on whether D is used for updation of the prior first or whether D' is used. This updation discrepancy depends on the particular model function that defines the likelihood. 

In most models, the probability of the data p(D|theta) does not depend in any other way on the data. In these situations, under the condition of independance of the data D and D', one sees

p(D,D'|theta) = p(D|theta)*p(D'|theta)

This leads to invariance in the ordering of the data for the posterior. That is, if the likelihood function has no dependence on data ordering then the posterior also does not have any dependence on data ordering. Thus, for data order-invariant likelihood functions,

p(theta|D, D') = p(theta|D',D)

One way of thinking of this is that every datum is equally representative of the underlying process, regardless of when it was observed and of any data being observed before or after.

**The posterior is a compromise between the prior distribution and the likelihood function.** Or, in other words, it is a compromise between the prior and the data. The compromise favours the prior to the extent that the prior distribution is sharply peaked and the data are few. The compromise facours the likelihood function to the extent that the prior distribution is flat and the data are many. The form of the posterior distribution from the mathematical formula highlights this clearly. In the event that data are few, the effect of the data on the prior to generate the posterior is reduced and the compromise favours the prior. However, in the event that the prior chosen is flat and data avvailable is plenty, the likelihood function will be favoured.

### Influence of sample size on the posterior

In general, the more data there is, the more precise is the estimate of the parameters of the model. Larger sample sizes yield greater precision or certainty of estimation.

### Influence of the prior on the posterior

In general, whenever the prior distributionis relatively broad compared with the likelihood function, the prior has fairly little influence on the posterior. When a very sharp prior is used, it noticeable influences the posterior.

Bayesian inference is intuitively rational.
For a strongly informed prior that has been devised by genuine prior knowledge to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. On the other hand, for a weakly informed prior that spreads credibility over a wide range of parameter values, relatively little data is required to shift the peak of the posterior toward the data.

It can be advantageous and rational to start with an informed prior and a serious blunder not to use strong prior information when it is available. 
